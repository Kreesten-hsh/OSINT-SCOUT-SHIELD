# ARCHITECTURE TECHNIQUE : SCRAPING AUTOMATIQUE (LOT 4)

Ce document d√©taille le fonctionnement interne du moteur de surveillance automatique d√©ploy√© sur **OSINT-SCOUT & SHIELD**.

---

## 1. VUE D'ENSEMBLE (Le Flux de Donn√©es)

Le syst√®me fonctionne en **boucle continue** (Architecture "Event-Driven" pilot√©e par √©v√©nements via Redis).

```mermaid
graph TD
    A[üïí SCHEDULER] -->|1. D√©tecte Source Active| B(Redis Queue: 'osint_to_scan')
    B -->|2. Consomme T√¢che| C[üë∑ WORKER SCRAPER]
    
    subgraph "DOCKER: osint-scraper"
        C -->|3. Navige & Capture| D[üï∑Ô∏è Engine/Playwright]
        C -->|4. Analyse Contenu| E[üß† Processor/NLP]
        E -->|Lit| F[üìú rules.json]
    end
    
    C -->|5. Envoie Rapport| G(Redis Queue: 'osint_results')
    G -->|6. Consomme R√©sultat| H[üì• API RESULT CONSUMER]
    H -->|7. Stocke Alerte| I[(PostgreSQL DB)]
```

---

## 2. COMPOSANTS & FICHIERS CL√âS

Voici la liste exhaustive des fichiers qui font tourner cette machine, class√©s par √©tape du processus.

### √âTAPE 1 : L'ORDONNANCEMENT (Le Chef d'Orchestre)
C'est lui qui d√©cide **QUAND** lancer une analyse.
*   **Fichier :** `backend/app/workers/scheduler.py`
*   **R√¥le :**
    *   V√©rifie chaque minute la base de donn√©es (`monitoring_sources`).
    *   Si `derni√®re_analyse + fr√©quence < maintenant`, il cr√©e une t√¢che.
    *   Il pousse un message JSON dans la file Redis `osint_to_scan`.

### √âTAPE 2 : L'EX√âCUTION (L'Ouvrier)
C'est le conteneur Docker isol√© qui fait le travail sale (navigation web).
*   **Fichier Principal :** `scrapers/workers/worker.py`
    *   **R√¥le :** √âcoute Redis, re√ßoit la t√¢che, lance les moteurs, renvoie le r√©sultat.
*   **Moteur de Navigation :** `scrapers/runners/engine.py`
    *   **R√¥le :** Contr√¥le le navigateur, g√®re les cookies, scroll la page, prend la capture d'√©cran, calcule le Hash SHA-256.
*   **Moteur d'Analyse :** `scrapers/analysis/processor.py`
    *   **R√¥le :** Re√ßoit le texte brut, le nettoie (NLP), cherche les mots-cl√©s, calcule le Score de Risque.
*   **Configuration :** `scrapers/config/rules.json`
    *   **R√¥le :** Contient les r√®gles m√©tier (Ex: "Si mot 'Code' + 'Urgence' = Risque 80").

### √âTAPE 3 : L'INGESTION & STOCKAGE (Le Greffier)
C'est le retour vers l'application principale pour sauvegarde.
*   **Fichier :** `backend/app/workers/result_consumer.py`
    *   **R√¥le :** √âcoute le retour du worker.
    *   Si le rapport dit `is_alert: True`, il cr√©e une entr√©e dans la table `alerts`.
    *   Il met √† jour le statut de la t√¢che (`completed`) dans `scraping_runs`.
*   **Mod√®les de Donn√©es :** `backend/app/models/source.py` & `alert.py`
    *   **R√¥le :** D√©finit la structure des tables SQL (Alertes, Preuves, Runs).

---

## 3. SC√âNARIO DE VIE D'UNE ALERTE

Prenons un exemple concret :
1.  **08:00** : Le `scheduler.py` voit que la source *"Facebook Groupe Vente"* doit √™tre scann√©e. Il envoie l'ordre.
2.  **08:00:05** : Le `worker.py` re√ßoit l'ordre. Il appelle `engine.py`.
3.  **08:00:10** : `engine.py` visite la page, prend une capture, extrait le texte : *"Vends iPhone 15, paiement urgent par Mandat uniquement"*.
4.  **08:00:15** : `processor.py` lit ce texte. Il voit les mots "Urgent" et "Mandat" d√©finis dans `rules.json`.
5.  **08:00:16** : Le score calcul√© est de **85/100**. C'est une alerte.
6.  **08:00:17** : Le Worker envoie le paquet complet (JSON + Preuve Hash√©e) √† l'API.
7.  **08:00:18** : `result_consumer.py` re√ßoit le paquet, cr√©e l'alerte en base de donn√©es.
8.  **08:00:19** : L'alerte appara√Æt sur votre Dashboard.

---

## 4. COMMENT MODIFIER LE SYST√àME ?

*   **Pour changer la fr√©quence de scan** : Modifier la source via l'Interface ou la Base de Donn√©es (Table `monitoring_sources`).
*   **Pour affiner la d√©tection** : Modifier le fichier `scrapers/config/rules.json` (Ajouter des mots-cl√©s).
*   **Pour changer la m√©thode de capture** : Modifier le code Python `scrapers/runners/engine.py`.
